{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10 - Textual Data Analytics\n",
    "Complete the code with TODO tag.\n",
    "## 1. Feature Engineering\n",
    "In this exercise we will understand the functioning of TF/IDF ranking. Implement the feature engineering and its application, based on the code framework provided below.\n",
    "\n",
    "First we use textual data from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>849636868052275200</td>\n",
       "      <td>2017-04-05 14:56:29</td>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>848988730585096192</td>\n",
       "      <td>2017-04-03 20:01:01</td>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>848943072423497728</td>\n",
       "      <td>2017-04-03 16:59:35</td>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>848935705057280001</td>\n",
       "      <td>2017-04-03 16:30:19</td>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848416049573658624</td>\n",
       "      <td>2017-04-02 06:05:23</td>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id           created_at  \\\n",
       "0  849636868052275200  2017-04-05 14:56:29   \n",
       "1  848988730585096192  2017-04-03 20:01:01   \n",
       "2  848943072423497728  2017-04-03 16:59:35   \n",
       "3  848935705057280001  2017-04-03 16:30:19   \n",
       "4  848416049573658624  2017-04-02 06:05:23   \n",
       "\n",
       "                                                text  \n",
       "0  b'And so the robots spared humanity ... https:...  \n",
       "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...  \n",
       "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'  \n",
       "3                b'Stormy weather in Shortville ...'  \n",
       "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('elonmusk_tweets.csv')\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Text Normalization\n",
    "Now we need to normalize text by stemming, tokenizing, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bforin2020',\n",
       " 'waltmossberg',\n",
       " 'mim',\n",
       " 'defcon5',\n",
       " 'exactli',\n",
       " 'tesla',\n",
       " 'is',\n",
       " 'absurdli',\n",
       " 'overvalu',\n",
       " 'if',\n",
       " 'base',\n",
       " 'on',\n",
       " 'the',\n",
       " 'past',\n",
       " 'but',\n",
       " 'that',\n",
       " 'irrxe2x80xa6',\n",
       " 'httpstcoqqctqkzgml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(document):\n",
    "    # TODO: remove punctuation\n",
    "    text = document.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    ret = \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "    return ret\n",
    "\n",
    "original_documents = [x.strip() for x in data['text']] \n",
    "documents = [normalize(d).split() for d in original_documents]\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the normalization is still not perfect. Please feel free to improve upon (OPTIONAL), e.g. https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Implement TF-IDF\n",
    "Now you need to implement TF-IDF, including creating the vocabulary, computing term frequency, and normalizing by tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tesla', 287)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['brt', 'tesla', 'spacex', 'model', 'thi']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten all the documents\n",
    "flat_list = [word for doc in documents for word in doc]\n",
    "\n",
    "# TODO: remove stop words from the vocabulary\n",
    "words = [word for word in flat_list if word not in stopwords.words('english')]\n",
    "\n",
    "# we take the 500 most common words only\n",
    "counts = Counter(words)\n",
    "vocabulary = counts.most_common(500)\n",
    "print([x for x in vocabulary if x[0] == 'tesla'])\n",
    "vocabulary = [x[0] for x in vocabulary]\n",
    "assert len(vocabulary) == 500\n",
    "\n",
    "# vocabulary.sort()\n",
    "vocabulary[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['tesla', 'exactli'], dtype='<U17'), array([1, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf(vocabulary, documents):\n",
    "    matrix = [0] * len(documents)\n",
    "    for i, document in enumerate(documents):\n",
    "        counts = Counter(document)\n",
    "        matrix[i] = [0] * len(vocabulary)\n",
    "        for j, term in enumerate(vocabulary):\n",
    "            matrix[i][j] = counts[term]\n",
    "    return matrix\n",
    "\n",
    "tf = tf(vocabulary, documents)\n",
    "np.array(vocabulary)[np.where(np.array(tf[1]) > 0)], np.array(tf[1])[np.where(np.array(tf[1]) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brt': 1.7579288672136189, 'tesla': 2.2951632529529067, 'spacex': 2.578161476092261, 'model': 2.6408325830550368, 'thi': 2.6309315120723253, 'amp': 2.838192017213532, 'car': 2.900712374194866, 'teslamotor': 2.8942814838645754, 'launch': 3.0241565652859874, 'like': 3.0766030406585303, 'rocket': 3.0613355685277415, 'wa': 3.1566457483320667, 'good': 3.1078555841626345, 'land': 3.2713086566522063, 'dragon': 3.3694265126107297, 'ye': 3.338967305126021, 'need': 3.422348914065072, 'time': 3.455501121381973, 'falcon': 3.4897901948606047, 'test': 3.587428664424521, 'make': 3.562110856440231, 'next': 3.537418243849859, 'year': 3.587428664424521, 'ha': 3.574689638647091, 'first': 3.587428664424521, 'go': 3.587428664424521, 'thank': 3.574689638647091, 'look': 3.6134041508277814, 'elonmusk': 3.626649377577802, 'new': 3.6536780499657215, 'work': 3.667471372098057, '3': 3.724629785938006, 'one': 3.7394448717231463, 'week': 3.7394448717231463, 'would': 3.754482749087687, '9': 3.724629785938006, 'x': 3.817003106069021, 'peopl': 3.833263626940801, 'space': 3.8666000472083932, 'get': 3.849792928892012, 'great': 3.9368043058816418, 'autopilot': 3.9368043058816418, 'use': 3.955153444549838, 'mar': 3.955153444549838, 'flight': 3.955153444549838, 'drive': 3.955153444549838, 'w': 3.992893772532685, 'nasa': 3.992893772532685, 'day': 3.992893772532685, 'btesla': 3.955153444549838, 'onli': 3.992893772532685, 'much': 3.992893772532685, 'best': 4.012311858389786, 'engin': 4.052317193003486, 'come': 4.012311858389786, 'today': 4.032114485685967, 'also': 4.032114485685967, 'product': 4.032114485685967, 'lot': 4.093989889404054, 'realli': 4.072936480206222, 'solar': 4.093989889404054, 'mile': 4.093989889404054, 'us': 4.072936480206222, 'love': 4.072936480206222, 'way': 4.115496094625017, 'right': 4.115496094625017, 'stage': 4.2064678728307445, 'system': 4.137475001343793, 'live': 4.115496094625017, 'back': 4.137475001343793, 'think': 4.137475001343793, 'chang': 4.159947857195851, 'take': 4.137475001343793, 'dont': 4.137475001343793, 'articl': 4.137475001343793, 'better': 4.159947857195851, 'orbit': 4.2064678728307445, '2': 4.1829373754205506, 'hope': 4.159947857195851, 'actual': 4.1829373754205506, 'soon': 4.1829373754205506, 'world': 4.2305654244098045, 'probabl': 4.2064678728307445, 'fire': 4.2305654244098045, 'high': 4.2305654244098045, 'ever': 4.255258037000176, 'power': 4.2305654244098045, 'futur': 4.2305654244098045, 'mission': 4.2305654244098045, 'show': 4.255258037000176, 'still': 4.280575844984466, 'im': 4.360618552658003, 'say': 4.333219578469889, 'station': 4.280575844984466, 'team': 4.333219578469889, 'video': 4.306551331387727, 'human': 4.333219578469889, 'open': 4.333219578469889, 'bmodel': 4.333219578469889, 'read': 4.333219578469889, 'mani': 4.417776966497951, 'batteri': 4.360618552658003, 'want': 4.417776966497951, 'point': 4.3887894296246985, 'mayb': 4.3887894296246985, 'updat': 4.447629929647633, 'yeah': 4.417776966497951, 'see': 4.417776966497951, 'tri': 4.417776966497951, 'watch': 4.447629929647633, 'month': 4.447629929647633, 'softwar': 4.417776966497951, 'ai': 4.5429401094519575, 'start': 4.447629929647633, 'veri': 4.510150286628966, 'last': 4.447629929647633, 'ani': 4.447629929647633, 'owner': 4.447629929647633, 'supercharg': 4.447629929647633, 'climat': 4.478401588314386, 'thing': 4.478401588314386, 'order': 4.5429401094519575, 'improv': 4.478401588314386, 'long': 4.478401588314386, 'end': 4.478401588314386, 'well': 4.510150286628966, '5': 4.510150286628966, 'know': 4.510150286628966, 'bjust': 4.510150286628966, 'electr': 4.510150286628966, 'tomorrow': 4.510150286628966, 'awesom': 4.510150286628966, 'help': 4.5429401094519575, 'deploy': 4.5429401094519575, 'could': 4.5429401094519575, 'energi': 4.576841661127639, 'satellit': 4.5429401094519575, 'hyperloop': 4.89961505339069, 'via': 4.5429401094519575, 'exactli': 4.576841661127639, 'due': 4.576841661127639, '1': 4.611932980938908, 'cape': 4.576841661127639, 'target': 4.576841661127639, 'whi': 4.576841661127639, 'bthe': 4.576841661127639, 'vehicl': 4.611932980938908, 'call': 4.611932980938908, 'true': 4.611932980938908, 'almost': 4.611932980938908, 'sure': 4.611932980938908, 'even': 4.611932980938908, 'hi': 4.648300625109783, 'complet': 4.611932980938908, 'la': 4.611932980938908, 'et': 4.7252616662459115, 'design': 4.648300625109783, 'happen': 4.648300625109783, 'support': 4.648300625109783, 'cool': 4.648300625109783, 'unveil': 4.648300625109783, 'full': 4.686040953092631, 'advanc': 4.7252616662459115, 'someth': 4.686040953092631, 'appreci': 4.686040953092631, 'announc': 4.686040953092631, 'canaver': 4.686040953092631, 'befor': 4.686040953092631, 'life': 4.686040953092631, 'thrust': 4.686040953092631, 'alreadi': 4.7252616662459115, 'cours': 4.7252616662459115, 'plan': 4.7252616662459115, 'ship': 4.7252616662459115, 'tax': 4.808643275184963, 'min': 4.7252616662459115, 'compani': 4.766083660766167, 'hour': 4.808643275184963, 'speed': 4.808643275184963, 'done': 4.808643275184963, 'kid': 4.808643275184963, 'review': 4.766083660766167, 'may': 4.766083660766167, 'super': 4.766083660766167, 'made': 4.853095037755796, 'california': 4.766083660766167, 'weather': 4.808643275184963, 'releas': 4.853095037755796, 'alway': 4.853095037755796, 'charg': 4.808643275184963, 'earth': 4.853095037755796, 'air': 4.808643275184963, 'attempt': 4.808643275184963, 'big': 4.808643275184963, 'droneship': 4.853095037755796, 'real': 4.808643275184963, '1st': 4.853095037755796, 'heavi': 4.853095037755796, 'httpsxe2x80xa6': 4.853095037755796, 'allow': 4.853095037755796, 'build': 4.853095037755796, 'custom': 4.853095037755796, 'post': 4.853095037755796, 'person': 4.853095037755796, 'servic': 4.853095037755796, 'hard': 4.853095037755796, 'rate': 4.89961505339069, 'gener': 4.853095037755796, 'v': 4.89961505339069, 'leg': 4.89961505339069, 'ga': 4.948405217560121, 'import': 4.89961505339069, 'bfalcon': 4.89961505339069, 'low': 4.948405217560121, '6': 4.89961505339069, 'pad': 4.89961505339069, 'rang': 4.89961505339069, 'motor': 4.999698511947672, 'vs': 4.89961505339069, 'stori': 4.89961505339069, 'enough': 4.89961505339069, 'sec': 4.999698511947672, 'safeti': 4.948405217560121, 'trip': 4.948405217560121, 'wont': 4.948405217560121, 'cant': 4.948405217560121, 'turn': 4.948405217560121, 'upper': 4.948405217560121, 'success': 4.948405217560121, 'piec': 4.999698511947672, 'version': 5.053765733217948, 'happi': 5.053765733217948, 'side': 4.948405217560121, 'less': 4.999698511947672, 'caus': 4.948405217560121, 'give': 4.948405217560121, 'price': 4.999698511947672, 'close': 4.948405217560121, 'sound': 4.999698511947672, 'spacest': 4.948405217560121, 'record': 4.948405217560121, 'mph': 5.110924147057896, 'agre': 4.948405217560121, 'second': 4.948405217560121, 'thought': 4.948405217560121, 'later': 4.999698511947672, 'part': 5.053765733217948, 'hold': 4.948405217560121, 'seem': 4.948405217560121, 'matter': 4.999698511947672, 'creat': 4.999698511947672, 'httpstcoxe2x80xa6': 4.999698511947672, 'center': 5.053765733217948, 'moon': 4.999698511947672, 'risk': 4.999698511947672, 'ok': 4.999698511947672, 'elon': 4.999698511947672, 'detail': 4.999698511947672, 'didnt': 4.999698511947672, 'home': 5.053765733217948, 'cover': 4.999698511947672, 'perform': 4.999698511947672, 'sever': 4.999698511947672, 'doe': 4.999698511947672, 'state': 4.999698511947672, 'data': 4.999698511947672, 'report': 4.999698511947672, 'road': 4.999698511947672, 'night': 4.999698511947672, 'bwill': 4.999698511947672, 'max': 5.053765733217948, 'dealer': 5.110924147057896, 'upgrad': 5.110924147057896, 'achiev': 5.053765733217948, '60': 5.053765733217948, 'around': 5.053765733217948, 'countri': 5.053765733217948, 'pack': 5.053765733217948, 'ask': 5.053765733217948, 'bam': 5.053765733217948, 'bit': 5.053765733217948, 'spaceship': 5.053765733217948, 'doesnt': 5.110924147057896, 'isnt': 5.053765733217948, 'never': 5.053765733217948, '10': 5.053765733217948, 'tweet': 5.110924147057896, 'control': 5.110924147057896, 'move': 5.053765733217948, 'feel': 5.053765733217948, 'said': 5.053765733217948, 'find': 5.053765733217948, 'camera': 5.053765733217948, 'far': 5.053765733217948, 'earli': 5.053765733217948, 'top': 5.053765733217948, 'recommend': 5.053765733217948, 'return': 5.053765733217948, 'news': 5.053765733217948, 'name': 5.053765733217948, 'booster': 5.053765733217948, 'littl': 5.110924147057896, 'photo': 5.110924147057896, 'vertic': 5.110924147057896, 'bif': 5.110924147057896, '4': 5.171548768874331, 'amaz': 5.110924147057896, 'took': 5.110924147057896, 'definit': 5.110924147057896, 'technolog': 5.110924147057896, 'cost': 5.110924147057896, 'fix': 5.110924147057896, 'issu': 5.110924147057896, 'hear': 5.110924147057896, 'pleas': 5.110924147057896, 'carbon': 5.171548768874331, 'set': 5.110924147057896, 'possibl': 5.171548768874331, 'action': 5.110924147057896, 'wrong': 5.110924147057896, 'meet': 5.110924147057896, 'everi': 5.171548768874331, 'roof': 5.236087290011903, 'rear': 5.305080161498854, 'bgood': 5.110924147057896, 'head': 5.171548768874331, 'blaunch': 5.110924147057896, 'texa': 5.110924147057896, 'believ': 5.171548768874331, 'option': 5.171548768874331, 'problem': 5.171548768874331, 'fast': 5.171548768874331, 'consum': 5.171548768874331, 'mean': 5.171548768874331, 'confirm': 5.171548768874331, '100': 5.171548768874331, 'free': 5.171548768874331, 'bspacex': 5.171548768874331, 'without': 5.171548768874331, 'might': 5.171548768874331, 'two': 5.171548768874331, 'spacecraft': 5.171548768874331, 'roll': 5.236087290011903, 'fli': 5.171548768874331, 'small': 5.171548768874331, 'anyth': 5.171548768874331, 'wire': 5.171548768874331, 'bad': 5.171548768874331, 'track': 5.171548768874331, 'tonight': 5.171548768874331, 'expect': 5.171548768874331, 'goe': 5.171548768874331, 'must': 5.236087290011903, 'deliv': 5.171548768874331, 'got': 5.171548768874331, 'ride': 5.171548768874331, 'xe2x80xa6': 5.171548768874331, 'short': 5.171548768874331, 'sale': 5.171548768874331, 'public': 5.305080161498854, 'coast': 5.379188133652576, 'pass': 5.171548768874331, 'openai': 5.305080161498854, 'bring': 5.236087290011903, 'fulli': 5.236087290011903, 'minut': 5.236087290011903, 'window': 5.236087290011903, 'three': 5.305080161498854, 'yet': 5.236087290011903, 'care': 5.236087290011903, 'ive': 5.236087290011903, 'talk': 5.236087290011903, 'line': 5.305080161498854, 'complex': 5.305080161498854, 'total': 5.236087290011903, 'save': 5.236087290011903, 'sens': 5.236087290011903, 'direct': 5.236087290011903, 'idea': 5.305080161498854, 'gigafactori': 5.236087290011903, 'weekend': 5.236087290011903, 'morn': 5.236087290011903, 'stop': 5.236087290011903, 'critic': 5.236087290011903, 'pod': 5.236087290011903, 'musk': 5.236087290011903, 'water': 5.305080161498854, 'increas': 5.236087290011903, 'bvicent': 5.236087290011903, '14': 5.236087290011903, 'bidaacarmack': 5.236087290011903, 'solarc': 5.236087290011903, 'interest': 5.236087290011903, 'bgreat': 5.236087290011903, 'size': 5.236087290011903, 'auto': 5.305080161498854, 'door': 5.379188133652576, 'physic': 5.305080161498854, 'extrem': 5.379188133652576, 'question': 5.305080161498854, 'worth': 5.305080161498854, 'china': 5.305080161498854, 'away': 5.305080161498854, 'limit': 5.379188133652576, 'hit': 5.379188133652576, 'machin': 5.379188133652576, 'reason': 5.305080161498854, 'play': 5.379188133652576, 'place': 5.305080161498854, 'level': 5.305080161498854, 'case': 5.305080161498854, 'abort': 5.305080161498854, 'fair': 5.305080161498854, 'break': 5.305080161498854, 'mention': 5.305080161498854, 'forc': 5.305080161498854, 'tmrw': 5.305080161498854, 'radar': 5.305080161498854, 'bi': 5.305080161498854, 'seat': 5.546242218315742, 'veloc': 5.305080161498854, 'death': 5.459230841326113, 'bmi': 5.305080161498854, 'oper': 5.305080161498854, 'bwe': 5.305080161498854, 'reentri': 5.305080161498854, 'brocket': 5.305080161498854, 'httptcoxe2x80xa6': 5.305080161498854, 'thruster': 5.379188133652576, 'develop': 5.379188133652576, 'crazi': 5.546242218315742, 'def': 5.379188133652576, 'ad': 5.379188133652576, 'deep': 5.379188133652576, 'learn': 5.379188133652576, 'though': 5.379188133652576, 'keep': 5.379188133652576, 'seri': 5.459230841326113, 'featur': 5.459230841326113, 'game': 5.379188133652576, 'number': 5.379188133652576, 'hous': 5.379188133652576, 'burn': 5.379188133652576, 'check': 5.379188133652576, 'term': 5.459230841326113, 'pressur': 5.379188133652576, 'provid': 5.379188133652576, 'anoth': 5.379188133652576, 'beauti': 5.379188133652576, 'law': 5.546242218315742, 'job': 5.379188133652576, 'requir': 5.379188133652576, 'webcast': 5.379188133652576, 'belonmusk': 5.379188133652576, '30': 5.379188133652576, 'media': 5.379188133652576, 'view': 5.379188133652576, 'reach': 5.379188133652576, 'experi': 5.379188133652576, 'suggest': 5.379188133652576, 'bdragon': 5.379188133652576, 'ocean': 5.379188133652576, 'babout': 5.379188133652576, 'entir': 5.459230841326113, '20': 5.459230841326113, 'mode': 5.459230841326113, 'current': 5.459230841326113, 'travel': 5.459230841326113, 'research': 5.459230841326113, 'norway': 5.546242218315742, 'white': 5.459230841326113, 'wish': 5.459230841326113, 'hardwar': 5.459230841326113, 'selfdriv': 5.459230841326113, 'forward': 5.459230841326113, 'welcom': 5.459230841326113, 'liftoff': 5.459230841326113, 'note': 5.459230841326113, 'excit': 5.459230841326113, 'bthank': 5.459230841326113, 'chanc': 5.546242218315742, 'fuel': 5.459230841326113, 'bbtw': 5.459230841326113, 'fine': 5.459230841326113, 'blook': 5.459230841326113, 'publish': 5.459230841326113, 'million': 5.459230841326113, 'readi': 5.459230841326113, 'heard': 5.459230841326113, 'ft': 5.641552398120067, 'rais': 5.459230841326113, 'race': 5.459230841326113, 'taken': 5.459230841326113, 'buy': 5.459230841326113, 'wind': 5.459230841326113, 'favor': 5.459230841326113, 'clear': 5.459230841326113, 'store': 5.459230841326113, 'includ': 5.459230841326113, 'bthi': 5.459230841326113}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7579288672136189,\n",
       " 2.2951632529529067,\n",
       " 2.578161476092261,\n",
       " 2.6408325830550368,\n",
       " 2.6309315120723253]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idf(vocabulary, documents):\n",
    "    \"\"\"TODO: compute IDF, storing values in a dictionary\"\"\"\n",
    "    idf = {}\n",
    "    for word in vocabulary:\n",
    "        idf[word] = np.log(len(documents)/ float(np.sum([1 if word in document else 0 for document in documents]) + 1))\n",
    "    return idf\n",
    "\n",
    "idf = idf(vocabulary, documents)\n",
    "print(idf)\n",
    "[idf[key] for key in vocabulary[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['tesla', 'exactli'], dtype='<U17'), array([2.29516325, 4.57684166]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def vectorize(documents, vocabulary, idf, tf):\n",
    "#     document_vectors = [0] * len(documents)\n",
    "#     for i, document in enumerate(documents):\n",
    "#         document_vectors[i] = [0] * len(vocabulary)\n",
    "#         for j, term in enumerate(vocabulary):\n",
    "#             document_vectors[i][j] = idf[term] * tf[i][j]\n",
    "#     return document_vectors\n",
    "\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]\n",
    "    return vector\n",
    "\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "np.array(vocabulary)[np.where(np.array(document_vectors[1]) > 0)], np.array(document_vectors[1])[np.where(np.array(document_vectors[1]) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Compare the results with the reference implementation of scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the scikit-learn library. As you can see that, the way we do text normalization affects the result. Feel free to further improve upon (OPTIONAL), e.g. https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http', 163.54366542841234), ('https', 151.85039944652075), ('rt', 112.61998731390989), ('tesla', 95.96401470715628), ('xe2', 88.20944486346477)]\n",
      "testla 0.3495243100660956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english', max_features=500)\n",
    "\n",
    "features = tfidf.fit(original_documents)\n",
    "corpus_tf_idf = tfidf.transform(original_documents) \n",
    "\n",
    "sum_words = corpus_tf_idf.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
    "print(sorted(words_freq, key = lambda x: x[1], reverse=True)[:5])\n",
    "print('testla', corpus_tf_idf[1, features.vocabulary_['tesla']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.  Apply TF-IDF for information retrieval\n",
    "We can use the vector representation of documents to implement an information retrieval system. We test with the query $Q$ = \"tesla nasa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 documents\n",
      "0 b'Would also like to congratulate @Boeing, fellow winner of the @NASA commercial crew program'\n",
      "1 b'RT @Commercial_Crew: .@NASA orders @SpaceX crew mission to @Space_Station. Read details at https://t.co/XGSEOSVuYZ https://t.co/N42GidqN68'\n",
      "2 b'@atduarte From anywhere'\n",
      "3 b'Compliment from Jeffrey is much appreciated, but the people of Tesla deserve all the credit'\n",
      "4 b'Cool! https://t.co/HOe0K7qkuk'\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"TODO: compute cosine similarity\"\"\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    v1 = np.array(v1)\n",
    "    v2 = np.array(v2)\n",
    "    \n",
    "    result = (np.sum(np.dot(v1, v2))) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    return result\n",
    "\n",
    "def search_vec(query, k, vocabulary, stemmer, document_vectors, original_documents):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    \n",
    "    # TODO: rank the documents by cosine similarity\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(document_vectors))]\n",
    "    scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    print('Top-{0} documents'.format(k))\n",
    "    for i in range(k):\n",
    "        print(i, original_documents[scores[i][1]])\n",
    "\n",
    "query = \"tesla nasa\"\n",
    "stemmer = PorterStemmer()\n",
    "search_vec(query, 5, vocabulary, stemmer, document_vectors, original_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the scikit-learn library to do the retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 documents\n",
      "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
      "1 b\"SpaceX could not do this without NASA. Can't express enough appreciation. https://t.co/uQpI60zAV7\"\n",
      "2 b'@NASA launched a rocket into the northern lights http://t.co/tR2cSeMV'\n",
      "3 b'Whatever happens today, we could not have done it without @NASA, but errors are ours alone and me most of all.'\n",
      "4 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n"
     ]
    }
   ],
   "source": [
    "new_features = tfidf.transform([query])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, corpus_tf_idf).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "\n",
    "topk = 5\n",
    "print('Top-{0} documents'.format(topk))\n",
    "for i in range(topk):\n",
    "    print(i, original_documents[related_docs_indices[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
