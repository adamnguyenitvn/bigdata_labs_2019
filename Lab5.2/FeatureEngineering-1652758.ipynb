{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "2.1_feature_engineering (hcmut).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL53eBjDNpj1",
        "colab_type": "text"
      },
      "source": [
        "# Lab 10 - Textual Data Analytics\n",
        "Complete the code with TODO tag.\n",
        "## 1. Feature Engineering\n",
        "In this exercise we will understand the functioning of TF/IDF ranking. Implement the feature engineering and its application, based on the code framework provided below.\n",
        "\n",
        "First we use textual data from Twitter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "XlNXIVxPNpj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "3a41d21b-db98-4a5e-b272-0c62adcae309"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv('elonmusk_tweets.csv')\n",
        "print(len(data))\n",
        "data.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>849636868052275200</td>\n",
              "      <td>2017-04-05 14:56:29</td>\n",
              "      <td>b'And so the robots spared humanity ... https:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>848988730585096192</td>\n",
              "      <td>2017-04-03 20:01:01</td>\n",
              "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>848943072423497728</td>\n",
              "      <td>2017-04-03 16:59:35</td>\n",
              "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>848935705057280001</td>\n",
              "      <td>2017-04-03 16:30:19</td>\n",
              "      <td>b'Stormy weather in Shortville ...'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>848416049573658624</td>\n",
              "      <td>2017-04-02 06:05:23</td>\n",
              "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ...                                               text\n",
              "0  849636868052275200  ...  b'And so the robots spared humanity ... https:...\n",
              "1  848988730585096192  ...  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...\n",
              "2  848943072423497728  ...      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'\n",
              "3  848935705057280001  ...                b'Stormy weather in Shortville ...'\n",
              "4  848416049573658624  ...  b\"@DaveLeeBBC @verge Coal is dying due to nat ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kmYZhV1Npk7",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Text Normalization\n",
        "Now we need to normalize text by stemming, tokenizing, and removing stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "O5FfM5_ZNpk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0e2b776e-4df4-4f92-de04-64105564447a"
      },
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "import pprint \n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "7VvAASzYNplA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88940bd1-4c6b-4f12-b485-e47f59e4e7ce"
      },
      "source": [
        "def normalize(document):\n",
        "    # TODO: remove punctuation\n",
        "    text = document.translate(str.maketrans('','',string.punctuation))\n",
        "    \n",
        "    # tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    \n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    ret = \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
        "    return ret\n",
        "\n",
        "original_documents = [x.strip() for x in data['text']] \n",
        "documents = [normalize(d).split() for d in original_documents]\n",
        "documents[0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['band', 'so', 'the', 'robot', 'spare', 'human', 'httpstcov7jujqwfcv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSfRhIYbNplD",
        "colab_type": "text"
      },
      "source": [
        "As you can see that the normalization is still not perfect. Please feel free to improve upon (OPTIONAL), e.g. https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol-HWNAINplD",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. Implement TF-IDF\n",
        "Now you need to implement TF-IDF, including creating the vocabulary, computing term frequency, and normalizing by tf-idf weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "dFvnOO0QNplE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "10d7896d-7940-4237-ea64-eb508b8157ce"
      },
      "source": [
        "# Flatten all the documents\n",
        "flat_list = [word for doc in documents for word in doc]\n",
        "\n",
        "# TODO: remove stop words from the vocabulary\n",
        "words = filter(lambda word: word not in set(stopwords.words('english')), flat_list)\n",
        "\n",
        "# we take the 500 most common words only\n",
        "counts = Counter(words)\n",
        "vocabulary = counts.most_common(500)\n",
        "print([x for x in vocabulary if x[0] == 'tesla'])\n",
        "vocabulary = [x[0] for x in vocabulary]\n",
        "assert len(vocabulary) == 500\n",
        "\n",
        "# vocabulary.sort()\n",
        "vocabulary[:5]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('tesla', 287)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brt', 'tesla', 'spacex', 'model', 'thi']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Avt74iFMNplI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf6fc9b3-7bcb-494d-b4f5-02adbb27275c"
      },
      "source": [
        "def tf(vocabulary, documents):\n",
        "    matrix = [0] * len(documents)\n",
        "    for i, document in enumerate(documents):\n",
        "        counts = Counter(document)\n",
        "        matrix[i] = [0] * len(vocabulary)\n",
        "        for j, term in enumerate(vocabulary):\n",
        "            matrix[i][j] = counts[term]\n",
        "    return matrix\n",
        "\n",
        "tf = tf(vocabulary, documents)\n",
        "np.array(vocabulary)[np.where(np.array(tf[1]) > 0)], np.array(tf[1])[np.where(np.array(tf[1]) > 0)]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['tesla', 'exactli'], dtype='<U17'), array([1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "0RkLTP3nNplK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a246502b-a925-4d98-ed8a-5532179d189a"
      },
      "source": [
        "def idf(vocabulary, documents):\n",
        "    \"\"\"TODO: compute IDF, storing values in a dictionary\"\"\"\n",
        "    idf = {}\n",
        "    \n",
        "    for word in vocabulary:\n",
        "        documents_with_word = 0\n",
        "        for document in documents:\n",
        "            if word in document:\n",
        "                documents_with_word += 1\n",
        "                \n",
        "        idf[word] = np.log(len(documents)/float(documents_with_word + 1))\n",
        "            \n",
        "    return idf\n",
        "\n",
        "idf = idf(vocabulary, documents)\n",
        "[idf[key] for key in vocabulary[:5]]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7579288672136189,\n",
              " 2.2951632529529067,\n",
              " 2.578161476092261,\n",
              " 2.6408325830550368,\n",
              " 2.6309315120723253]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "hyr43hbKNplM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5657e677-6495-4de9-9e1b-bf4757cffd65"
      },
      "source": [
        "# def vectorize(documents, vocabulary, idf, tf):\n",
        "#     document_vectors = [0] * len(documents)\n",
        "#     for i, document in enumerate(documents):\n",
        "#         document_vectors[i] = [0] * len(vocabulary)\n",
        "#         for j, term in enumerate(vocabulary):\n",
        "#             document_vectors[i][j] = idf[term] * tf[i][j]\n",
        "#     return document_vectors\n",
        "\n",
        "def vectorize(document, vocabulary, idf):\n",
        "    vector = [0]*len(vocabulary)\n",
        "    counts = Counter(document)\n",
        "\n",
        "    for i,term in enumerate(vocabulary):\n",
        "        vector[i] = idf[term] * counts[term]\n",
        "    return vector\n",
        "\n",
        "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
        "np.array(vocabulary)[np.where(np.array(document_vectors[1]) > 0)], np.array(document_vectors[1])[np.where(np.array(document_vectors[1]) > 0)]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['tesla', 'exactli'], dtype='<U17'), array([2.29516325, 4.57684166]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePJNlnDaNplO",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Compare the results with the reference implementation of scikit-learn library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPgBfWhTNplP",
        "colab_type": "text"
      },
      "source": [
        "Now we use the scikit-learn library. As you can see that, the way we do text normalization affects the result. Feel free to further improve upon (OPTIONAL), e.g. https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "SZg0iChiNplQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "200c4319-567c-464b-e828-e6ace337f898"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english', max_features=500)\n",
        "\n",
        "features = tfidf.fit(original_documents)\n",
        "corpus_tf_idf = tfidf.transform(original_documents) \n",
        "\n",
        "sum_words = corpus_tf_idf.sum(axis=0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
        "print(sorted(words_freq, key = lambda x: x[1], reverse=True)[:5])\n",
        "print('tesla', corpus_tf_idf[1, features.vocabulary_['tesla']])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('http', 163.54366542841234), ('https', 151.85039944652075), ('rt', 112.61998731390989), ('tesla', 95.96401470715628), ('xe2', 88.20944486346477)]\n",
            "tesla 0.3495243100660956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5hP-5uYNplS",
        "colab_type": "text"
      },
      "source": [
        "### 1.4.  Apply TF-IDF for information retrieval\n",
        "We can use the vector representation of documents to implement an information retrieval system. We test with the query $Q$ = \"tesla nasa\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "MIbA5AbhNplT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8415d9c5-fc54-4be4-c150-bb28abcda710"
      },
      "source": [
        "def cosine_similarity(v1,v2):\n",
        "    \"\"\"TODO: compute cosine similarity\"\"\"\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    v1 = np.array(v1)\n",
        "    v2 = np.array(v2)\n",
        "    \n",
        "    sumxx = np.sum(v1 ** 2)\n",
        "    sumyy = np.sum(v2 ** 2)\n",
        "    sumxy = np.sum(np.dot(v1, v2))\n",
        "\n",
        "    if sumxx * sumyy == 0:\n",
        "      result = 0\n",
        "    else:\n",
        "      result = sumxy/(np.sqrt(sumxx) * np.sqrt(sumyy))\n",
        "    return result\n",
        "\n",
        "def search_vec(query, k, vocabulary, stemmer, document_vectors, original_documents):\n",
        "    q = query.split()\n",
        "    q = [stemmer.stem(w) for w in q]\n",
        "    query_vector = vectorize(q, vocabulary, idf)\n",
        "    \n",
        "    # TODO: rank the documents by cosine similarity\n",
        "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(document_vectors))]\n",
        "    scores.sort(key=lambda x: x[0], reverse=True)\n",
        "    \n",
        "    print('Top-{0} documents'.format(k))\n",
        "    for i in range(k):\n",
        "        print(i, original_documents[scores[i][1]])\n",
        "\n",
        "query = \"tesla nasa\"\n",
        "stemmer = PorterStemmer()\n",
        "search_vec(query, 5, vocabulary, stemmer, document_vectors, original_documents)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top-5 documents\n",
            "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
            "1 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n",
            "2 b\"Deeply appreciate @NASA's faith in @SpaceX. We will do whatever it takes to make NASA and the American people proud.\"\n",
            "3 b'Would also like to congratulate @Boeing, fellow winner of the @NASA commercial crew program'\n",
            "4 b\"@astrostephenson We're aiming for late 2015, but NASA needs to have overlapping capability to be safe. Would do the same\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCUkT68DNplk",
        "colab_type": "text"
      },
      "source": [
        "We can also use the scikit-learn library to do the retrieval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "3ZCQXmxWNpll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "797cabf5-4023-49b4-a7c8-39d47fdbc9b4"
      },
      "source": [
        "new_features = tfidf.transform([query])\n",
        "\n",
        "cosine_similarities = linear_kernel(new_features, corpus_tf_idf).flatten()\n",
        "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
        "\n",
        "topk = 5\n",
        "print('Top-{0} documents'.format(topk))\n",
        "for i in range(topk):\n",
        "    print(i, original_documents[related_docs_indices[i]])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top-5 documents\n",
            "0 b'@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.'\n",
            "1 b\"SpaceX could not do this without NASA. Can't express enough appreciation. https://t.co/uQpI60zAV7\"\n",
            "2 b'@NASA launched a rocket into the northern lights http://t.co/tR2cSeMV'\n",
            "3 b'Whatever happens today, we could not have done it without @NASA, but errors are ours alone and me most of all.'\n",
            "4 b'RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}